<!DOCTYPE html>
<html lang="en">
	<head>
		<title>Some AI companies steal. Here is how you can stop them. ‚Äî Wenzels.Blog</title>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link href="/assets/style.css" rel="stylesheet">
		<link rel="stylesheet" href="/assets/hljs-vs2015.min.css" />
		<link rel="alternate" type="application/rss+xml" 
		  title="RSS Feed" 
		  href="/feed.xml" />
		<base href="/notes/" />
	</head>
	<body>
		<header>
			<div class="container">
				<h1 id="pageTitle"><a href="/" title="Home"><span role="img" aria-label="Home">üè°</span> <span class="viewport-hideOnXS">Wenzels.Blog</span></a></h1>
				<ol>
					<li><a href="/about">About</a></li>
					<li><a href="https://wenzels.design" target="_blank">Work</a></li>
					<li><a href="/feed.xml" target="_blank">RSS</a></li>
				</ol>
			</div>
		</header>
		<main>
	<div class="postHead">
		<h1>Some AI companies steal. Here is how you can stop them.</h1>
		<time datetime="Fri, 06 Oct 2023 02:00:00 +0000">Oct.‚Äà6, 2023</time>
	</div>
	<h2>What is the problem?</h2>
<p>Lately, I've been asking myself one question repeatedly: Do I mind if AI is trained with the text I write? My answer to that is: No, I don't mind. I should have asked myself a different question, though: Do I mind when AI companies use my data to train their models, profit from it, and pay me nothing in return? Yes, it bothers me.</p>
<p>Companies like OpenAI, Meta (Facebook), Alphabet (Google) and others are using the freely online available data to train their AI models. Yet they pay nothing to the authors and copyright holders of these texts. Often, the question of who has what rights over these texts is also unclear. AIs, with the help of so-called &quot;crawlers&quot;<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>, scour the Internet for information that they appropriate and present to their users, often without citing a source or holding the right over the text.</p>
<p>I want to stop AI companies from using my writing to their advantage without paying me. In this post I explain how I do that, and it's relatively simple. Yes, this blog is free for anyone to read, but I consider using this blog as training material for AI a form of piracy.</p>
<h2>The solution</h2>
<p>Maybe you think that if you just put the AI companies' crawlers' user agents<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> into your <code>robots.txt</code><sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>, the problem is solved. Indeed, if you want to trust the AI companies, that's the solution. However, I don't trust these companies anymore. So what we need is a solution that doesn't require trust.</p>
<p>Such a solution is an enforceable technique, one whose compliance you have under control: <code>.htaccess</code>. Htaccess is a mechanism used by the Apache web server ‚Äî probably the most widely used server software in the world. With <code>.htaccess</code> files you can customize the behavior of the server.</p>
<p>Instead of asking crawlers to stay away, you deny them access. Or in the case of the code below, you simply send them an empty page. If you have never heard of <code>.htaccess</code>, please read the [documentation on the Apache website](link: https://httpd.apache.org/docs/2.4/howto/htaccess.html) before blindly using the code below.</p>
<h3>The .htaccess-file</h3>
<pre><code class="hljs"><span class="hljs-section">&lt;IfModule mod_rewrite.c&gt;</span>
    <span class="hljs-attribute">RewriteEngine</span> <span class="hljs-literal">on</span>

    <span class="hljs-comment"># Block AI scrapers</span>
    <span class="hljs-attribute">RewriteCond</span> <span class="hljs-variable">%{HTTP_USER_AGENT}</span> ^.*(CCBot|ChatGPT-User|FacebookBot|Google-Extended|GPTBot|Omgilibot).*$<span class="hljs-meta"> [NC]</span>
    <span class="hljs-attribute">RewriteRule</span> .* -<span class="hljs-meta"> [F,L]</span>

<span class="hljs-section">&lt;/IfModule&gt;</span>
</code></pre>
<h2>How this .htaccess-Datei works</h2>
<p>If Apache is configured to allow redirecting user requests, we enable the necessary module:</p>
<pre><code class="hljs"><span class="hljs-section">&lt;IfModule mod_rewrite.c&gt;</span>
	<span class="hljs-attribute">RewriteEngine</span> <span class="hljs-literal">on</span>
</code></pre>
<p>Then, we specify the conditions under which we want to redirect requests:</p>
<pre><code class="hljs">    <span class="hljs-attribute">RewriteCond</span> <span class="hljs-variable">%{HTTP_USER_AGENT}</span> ^.*(CCBot|ChatGPT-User|FacebookBot|Google-Extended|GPTBot|Omgilibot).*$<span class="hljs-meta"> [NC]</span>
</code></pre>
<p>We use <code>RewriteCond</code> and with the variable <code>HTTP_USER_AGENT</code>. We test this variable for these keywords: CCBot, ChatGPT-User, FacebookBot, Google-Extended, GPTBot and Omgilibot. If one of the tests is successful, we change the behavior of the server and send an empty page (<code>-</code>) as a replacement for all (<code>.*</code>) addresses.</p>
<pre><code class="hljs">    <span class="hljs-attribute">RewriteRule</span> .* -<span class="hljs-meta"> [F,L]</span>
<span class="hljs-section">&lt;/IfModule&gt;</span>
</code></pre>
<h2>Is this solution effective?</h2>
<p>Probably, but it can be circumvented. If an AI company wanted to cloak its crawler, it could simply send a spoofed user agent. One that looks innocent. For example, a crawler could pretend to be the Google Chrome browser or Firefox, and the site would let it in merrily. For this blog, I'll take that risk for now.</p>
<h2>What else could be blocked?</h2>
<p>The blogger &quot;Fox&quot; published <a href="https://3xn.nl/projects/2023/10/06/bot-block-party/" class="external-link" target="_blank">a long list of user agents</a> which he blocks. Use this list if you want to go further than me. I decided not to include this list because I don't yet know what services these user agents represent.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>A crawler is an application that runs on a server and reads websites. It clicks on all the links on the page to find more websites to read. They were invented to provide data to search engines like Google. <a href="#fnref1" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn2" class="footnote-item"><p>The term user agent represents the type of computer that is currently accessing a website. Example: If you open this website using Google Chrome, the server will see your user agent indicate that you are using Chrome. When OpenAI accesses this website, the user agent is either &quot;GPTBot&quot; or &quot;ChatGPT-User&quot;. <a href="#fnref2" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn3" class="footnote-item"><p>The robots file can be used to tell a crawler such as Google or OpenAI that it is forbidden to read a particular page of a website. It is a text file that a website administrator can store on the site's server. Compliance is not guaranteed or enforceable. <a href="#fnref3" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
</ol>
</section>

	<p id="permalink"><a href="/notes/2023-10-06 Some AI companies steal Here is how you can stop them/">‚åûw‚åù</a></p>
</main>

	</body>
</html>
